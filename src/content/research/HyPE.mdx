---
title: Hypothesis Network Planned Exploration
skills: Python, PyTorch, Reinforcement Learning, Meta-Reinforcement Learning
dateRange: January 2024 - Present
link: https://github.com/JLZ22/HyPE-Experiments
--- 

# Summary
In model based meta-reinforcement learning, the agent first learns to learn from a distribution of tasks, then adapts to a new task with a as few steps as possible. During this adaptation phase, vanilla algorithms typically implement random exploration which can be inefficient in determining an accurate world model. Additionally, in highly random environments, accurately differentiating between different states in the latent space can be challenging. To address the issues, I am working on <u>Hypothesis Network Planned Exploration with Latent Distributions</u> (HyPE-LD) with Dr. YeXiang Xue and Ph.D. student Maxwell Jacobson. HyPE-LD is an action-planning algorithm which models states as distributions in the latent space. The algorithm gathers multiple sequences of state distributions by following a set of actions on multiple available models. By comparing these sequences, HyPE-LD aims to more efficiently deduce the most accurate world model, thereby improving the agent's ability to quickly adapt to new tasks in highly random environments.

# Existing Work
My contributions build upon the existing Hypothesis Network Planned Exploration (HyPE) algorithm which is detailed in [Hypothesis Network Planned Exploration for Rapid Meta-Reinforcement Learning Adaptation](https://arxiv.org/abs/2311.03701). HyPE takes a pool of models and strategically plans an exploration path to deduce which model is most accurate for the current task. This is very similar to how the scientific method works: by forming a hypothesis, testing it, and then refining the hypothesis based on the results. However, HyPE's exploration was tested on an entirely deterministic environment. My method extends the idea of HyPE to stochastic environments. 