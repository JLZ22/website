---
title: HyPE-LD
dateRange: January 2024 - Present
--- 

# Summary
In model based meta-reinforcement learning, the agent first learns to learn from a distribution of tasks, then adapts to a new task with a as few steps as possible. During this adaptation phase, vanilla algorithms typically implement random exploration which can be inefficient in determining an accurate world model. Additionally, in highly random environments, accurately differentiating between different states in the latent space can be challenging. To address the issues, I am working on HyPE-LD with Dr. YeXiang Xue and Ph.D. student Maxwell Jacobson. HyPE-LD is an action-planning algorithm which models states as distributions in the latent space. By comparing the distributions of different states in different models, HyPE-LD aims to more efficiently reduce the uncertainty in the world model and improve the agent's ability to adapt to new tasks in highly random environments.